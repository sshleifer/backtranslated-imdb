I really hate most end-of-the-world movies. They show what Hollywood people think of the rest of the world, and they clearly think we're a bunch of sadistic idiots (or at least watching sadistic idiots react to things is somehow amusing). I've been to LA many times, I have family that works in show business, and I just want to say that these are the last people we should be looking for for a reality check. Some disaster films at least paint a clever picture: Children of Men, 12 Monkeys, but generally the message is just "people will do anything to survive, everything is dark, sad and purposeless, we should all be ashamed of ourselves". <br /> Do not get me wrong, I like the idea of ​​a story that explores the social order system and test people's courage in the face of horror, and I believe that some people would act like this movie. portrayed, but sadistic idiots aside, I seriously doubt that society dissolves in every man for himself, that's just an insult.
