I really hate most of the world movies. They show what confused Hollywood people think of the rest of the world, and they clearly think we are a bunch of sadistic idiots (or at least seeing sadistic idiots reacting to things is somehow entertaining) . I have been to L.A. Many times, I have family working in the show business, and I just want to say that these are the * last * people we need to look for for a reality check. Some disaster films at least paint a beautiful picture: men's children, 12 monkeys, but usually the message is just "people will do everything to survive, everything is dark and sad and futile, we must all be ashamed of ourselves". /> <br /> Don't get me wrong, I like the idea of ​​a story that explores the system of social order and tests people's mettle in the horror of horror, and I think some people would act like this film portrayed, but sadistic idiots aside, I seriously doubt that society will just dissolve in every man for himself, it's just insulting.
