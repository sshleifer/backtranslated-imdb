I hate most movies in the world. They show what the Hollywood people think about the rest of the world, and they clearly believe we're a bunch of sadistic idiots (or at least watching sadistic idiots who react to things is kind of entertaining). I've been to L.A. many times, I have a family working in show business, and I just want to say that these are the * last * people we're looking for for a reality check. Some disaster films at least show a clever picture: Children of Men, 12 Monkeys, but usually the message is, "People will do anything to survive, everything is dark and sad and futile, we should all be ashamed of ourselves" /> <br /> Do not get me wrong, I like the idea of ​​a story that examines the dismantling of the system of social order and the testing of people's zeal in the face of horror. I think some people would behave like this movie Apart from the depicted sadistic idiots, I seriously doubt that society would dissolve in every man. That's just offensive.
