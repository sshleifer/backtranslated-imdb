I really hate the most recent world films. They show what poor Hollywood people have thought about the rest of the world, and clearly think that we are a bunch of sadistic idiots (or at least seeing how sadistic idiots react to things somehow fun). I was in LA. Many times, I have a family who works in a show business, and I just want to say that these are the people with whom we should look for reality. Some catastrophe films at least paint a smart picture: "Children of Men," 12 monkeys, but usually the message is only "people will do everything to survive, everything is dark, sad and meaningless, we should all be ashamed of ourselves." /> Do not misunderstand me, I like the idea of ​​a story that explores the throwing of a system of social order and testing people's feelings before horror, and I believe that some people would behave like this film portrayed, but sadistic idiots aside, seriously I doubt that society would simply fall apart in every human being, it's just an insult.
