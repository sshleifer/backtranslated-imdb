I really hate most movies from the end of the world. They show what Hollywood savvy people think of the rest of the world, and they clearly think that we are a bunch of sadistic idiots (or at least that seeing the sadistic idiots react to things is something entertaining). I have been to L.A. Many times, I have a family that works in the world of entertainment, and I just want to say that these are the * last * people we should go to for a reality check. Some films about disasters at least paint an intelligent image: Children of men, 12 monkeys, but generally the message is simply "people will do anything to survive, everything is dark, sad and without purpose, we should all be ashamed of ourselves". <Br /> <br /> Do not get me wrong, I like the idea of ​​a story that explores getting rid of the system of social order and testing the temper of people in the face of horror, and I think some people would act like this film portrayed, but Apart from the sadistic idiots, I seriously doubt that society dissolves in every man by itself, that is an insult.
