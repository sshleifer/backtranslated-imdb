I really hate the majority of the end of the movie world. They show that the exhausted Hollywood people think about the rest of the world, and they obviously think we're a bunch of sadistic idiots (or, at least, to watch sadistic idiots react to things, such as interest). I was L.A. many times, I have a family that works in show business, and I just want to say that the latter * people, we must look to for a reality check. Some movies accident, at least paint a clever painting: Children of men, 12 Monkeys, but as a rule, the message is simply "people will do anything to survive, all dark and sad and unnecessary, we should all be ashamed of yourselves," <br . /> <br /> Do not get me wrong, I like the idea of ​​stories that explores the dropping public order system and testing the courage of people in the face of horror, and I believe that some people will act as the movie portrayed, but a sadistic idiots aside I seriously doubt that society simply dissolve into every man for himself, it's just insulting.
