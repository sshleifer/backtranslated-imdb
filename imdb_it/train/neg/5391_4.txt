I really hate most movies in the world. They show what faded Hollywood people think of the rest of the world, and clearly think that we are a group of sadistic idiots (or at least that watching sadistic idiots react to things is somehow funny). I have been to L.A. many times, I have a family that works in the entertainment world, and I just want to say that these are the * last * people we should be looking for a reality check. Some films about disasters, at least, paint an intelligent image: Children of Men, 12 Monkeys, but usually the message is "people will do anything to survive, everything is dark, sad and aimless, we should all be ashamed of ourselves ". /> <br /> Don't get me wrong, I like the idea of ​​a story that you explore by throwing away the social order system and testing people's courage in the face of horror, and I think some people would behave like this movie portrayed, but apart from the sadistic idiots, I seriously doubt that society would dissolve in every man for himself, this is just an insult.
