I really hate most movies around the world. They show what Hollywood people think of the rest of the world, and they clearly think we're a group of sadistic idiots (or at least seeing sadistic idiots reacting to things is kind of entertaining). I have been to Los Angeles many times, I have a family working in show business and I just want to say that these are the * last * people we should be looking for a confrontation with reality. Some disaster movies paint at least one smart board: Children of Men, 12 Monkeys, but generally the message is simple: "people will do everything to survive, everything is dark, sad and aimless, we should all be ashamed of ourselves -Same ". <Br/> <br /> Do not get me wrong, I like the idea of ​​a story that explores overthrowing the social order system and testing people's courage in the face of horror, and I believe that some people would act like this film portrayed, but sadistic idiots aside, I seriously doubt that society would dissolve into each other for himself, which is insulting.
